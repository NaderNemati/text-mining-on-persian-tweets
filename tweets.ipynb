{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RnnLstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ItEqU_-dlA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "aa485694-3a37-44cf-e2bd-8fe5ab3e30bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPfMwV4qD8v_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaSJHZIhsbgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "eece890c-4a7b-49f7-c262-b89f1c274c85"
      },
      "source": [
        "!apt install zipfle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package zipfle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RWpUlMmsnDM"
      },
      "source": [
        "import zipfile\n",
        "data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_BsM92ftH5s"
      },
      "source": [
        "with zipfile.ZipFile('/content/drive/My Drive/tweet/twitter_sample_tweets.csv.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('./twitter_data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww-vfm3FtpS9"
      },
      "source": [
        "f = open(\"./twitter_data/twitter_sample_tweets.csv\", \"r\")\n",
        "df = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2byR7NVvl--"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q75G_Qtv4Lc"
      },
      "source": [
        "X_train, Y = train_test_split(df, test_size=0.9980, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbqPgli8_-aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "3f215188-8119-4eb4-c479-6d96b9c51a6a"
      },
      "source": [
        "from itertools import islice\n",
        "for row in islice(X_train, 10):\n",
        "  print(row)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"2019-01-05 22:42:51\",\"  من که دارم تمرین میکنم\",0,2,8438,97,356,1838\n",
            "\n",
            "\"2019-01-03 23:07:20\",\"اگر کسی عاشق #امام_زمانش باشد امکان ندارد ولایت فقیه و #رهبرش رو دوست نداشته باشد. #استاد_شجاعی \",1,8,39239,0,4,1\n",
            "\n",
            "\"2019-01-06 18:22:32\",\"خب چرا تو فیلم ها اسم گربه ها رو میذارن #کتی 🙄#کلمبوس\",1,15,201669,0,5,1\n",
            "\n",
            "\"2019-01-31 17:23:54\",\"RT :  سلام جناب نخست وزیر ما ایرانیان خواهان حکومتی که قوانین آن بر گرفته از کوروش و حاکمان آن خردورزان باهوشان و…\",4,0,27082,0,25,2\n",
            "\n",
            "\"2019-01-28 17:59:37\",\"RT : الان که شما راحت نشستید دارید بازی رو میبینید ، تراکتوریا تو شوکن که چپو تشویق کنن یا راستو\",29,0,334,34,59,119\n",
            "\n",
            "\"2019-01-28 11:23:07\",\"RT : اينايي كه رو اسم \"\"خليج فارس\"\" غيرتى ميشوند؛ و از نفت و صيد و گمرك خليج فارس ذره اى به آنها تعلق ندارد و همه اش در انحصار…\",18,0,17804,4622,1932,30769\n",
            "\n",
            "\"2019-01-08 16:09:00\",\"روز سه شنبه ۱۸دی  #غارت_شدگان_کاسپین  در #رشت، در اعتراض به غارت اموال و دارایی شان توسط موسسه غارتگر کاسپین، در مقابل دفتر این #موسسه تجمع کردند. \",1,3,81575,171,596,6534\n",
            "\n",
            "\"2019-01-09 18:36:15\",\" میگن گوگوش و رها اعتمادی ازدواج کردن\",0,1,6688,1263,209,32567\n",
            "\n",
            "\"2019-01-10 23:11:28\",\" داره با خشخاش حرف میزنه\",0,1,17020,1086,974,13069\n",
            "\n",
            "\"2019-01-17 22:12:06\",\" فقط بلدن از ما پول بکنند\",0,0,11943,7,9,135\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFTiTAi6Fx1W"
      },
      "source": [
        "data=[]\n",
        "import pandas as pd\n",
        "for i, line in enumerate(X_train):\n",
        "  data.append(line.split(','))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJCR0nCYHKdo"
      },
      "source": [
        "f.close()\n",
        "a=list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ney3esdhHm-V"
      },
      "source": [
        "for i in range(len(data)):\n",
        "  if len(data[i])==8:\n",
        "    a.append(data[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTu4ZQgHIKvm"
      },
      "source": [
        "triandata2=pd.DataFrame(a,columns=['col1','col2','col3','col4','col5','col6','col7','col8'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wzj-9VbPkOX"
      },
      "source": [
        "traindata=triandata2.loc[:]['col2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJmuNVErP5kd"
      },
      "source": [
        "import re\n",
        "reviews_ints = []\n",
        "for t in range (len(traindata)):\n",
        "  text = traindata[t]\n",
        "  text = re.sub(r'[^a-zA-Z0-9آ-ی۰-۹ ]', ' ', text)\n",
        "  text = text.strip()\n",
        "  traindata[t]=text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_6RR46qRoI3"
      },
      "source": [
        "words1=[]\n",
        "for i in range (0,len(traindata)):\n",
        "  words1.append(traindata[i].split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCAFL7iUUD8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0c0ec92-aee4-484e-dece-fbd03bdc480e"
      },
      "source": [
        "len(words1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31621"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltrfw2y8UF0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "c3395059-434d-4324-9225-aa485cb16fac"
      },
      "source": [
        "words1[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " 'دو',\n",
              " 'خاطره',\n",
              " 'زیبا',\n",
              " 'و',\n",
              " 'جالب',\n",
              " 'از',\n",
              " 'پهلوان',\n",
              " 'مسلم',\n",
              " 'اسکندر',\n",
              " 'فیلابی',\n",
              " 'از',\n",
              " 'جهان',\n",
              " 'پهلوان',\n",
              " 'تختیچرا',\n",
              " 'تختی',\n",
              " 'حاضر',\n",
              " 'نشد',\n",
              " 'نماینده',\n",
              " 'مجلس',\n",
              " 'شاه',\n",
              " 'شود',\n",
              " 'یک',\n",
              " 'خاطره',\n",
              " 'از',\n",
              " 'کمک']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnKF80P3UIHy"
      },
      "source": [
        "data=set()\n",
        "for i in range(0,len(words1)):\n",
        "  for j in range(0,len(words1[i])):\n",
        "    data.add(words1[i][j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYJznBEAVAlI"
      },
      "source": [
        "char_to_int = dict((c,i) for i, c in enumerate(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIHF6XJFVYF8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "ba070796-5367-476a-b013-5cf397aa1097"
      },
      "source": [
        "!apt install Tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package Tokenizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW_3HuUXVkWg"
      },
      "source": [
        "from keras_preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(1000)\n",
        "tokenizer.fit_on_texts(words1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A0QmVqgc5RR"
      },
      "source": [
        "def partitionlines(lines,maxlen):\n",
        "  data_x=[]\n",
        "  data_y=[]\n",
        "  for line in lines:\n",
        "    words = line\n",
        "    for i in range(0,maxlen):\n",
        "      if i >=len(words)-1:\n",
        "        break\n",
        "      data_x.append(words[0:i+1])\n",
        "      data_y.append(words[i+1])\n",
        "    for i in range(maxlen,len(words)):\n",
        "      if i >=len(words)-1:\n",
        "        break\n",
        "      data_x.append(words[i-maxlen+1:i+1])\n",
        "      data_y.append(words[i+1])\n",
        "  return data_x,data_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6FOgprYfBfH"
      },
      "source": [
        "maxlen = 10\n",
        "lines = tokenizer.texts_to_sequences(words1)\n",
        "data_x,data_y = partitionlines(lines,maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEjGZ_hrfiGL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "35a65ad5-85cc-4a85-dc01-09271fcdbba8"
      },
      "source": [
        "print('x1:',data_x[0])\n",
        "print('y1:',data_y[0])\n",
        "print('x2:',data_x[1])\n",
        "print('y2:',data_y[1])\n",
        "print('x3:',data_x[2])\n",
        "print('y3:',data_y[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1: [13]\n",
            "y1: 4\n",
            "x2: [13, 4]\n",
            "y2: 76\n",
            "x3: [13, 4, 76]\n",
            "y3: 105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPZSNVaruLf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b344b6d-362c-418a-c804-5fb9f8a091d8"
      },
      "source": [
        "from keras_preprocessing import sequence\n",
        "x_train = sequence.pad_sequences(data_x,maxlen=maxlen)\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(data_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpumW0p3vFnt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "362ec133-ec23-4cd0-bd2e-616ae53bac0b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.argmax(y_train[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEM-knqQvLb-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10083beb-8c99-458a-e0fb-0117664251f1"
      },
      "source": [
        "len(y_train[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UzvMr2kvYZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8a90e36-2885-4ba4-b2c2-a6198f466a74"
      },
      "source": [
        "max(data_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aNA_vUpvd7p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc3238bc-051e-4ca5-a308-aca90fc01ee3"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(310370, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUl4XHY0vhw6"
      },
      "source": [
        "import numpy as np\n",
        "np.max(y_train,axis=1)\n",
        "indices = np.arange(0,len(x_train))\n",
        "np.random.shuffle(indices)\n",
        "x_train_shuffled = x_train[indices]\n",
        "y_train_shuffled = y_train[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aavU3QowGEG"
      },
      "source": [
        "x_test = x_train_shuffled[-1000:]\n",
        "x_train_partial = x_train_shuffled[:-1000]\n",
        "y_test = y_train_shuffled[-1000:]\n",
        "y_train_partial = y_train_shuffled[:-1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3-PSddhxCZ1"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import  layers\n",
        "from keras import losses\n",
        "from keras import  metrics\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout, Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2-jPsqHxZ82"
      },
      "source": [
        "def getmodel():\n",
        "  model = Sequential()\n",
        "  model.add(layers.Embedding(1000, 128))\n",
        "  model.add(layers.LSTM(1026,return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(layers.LSTM(504))\n",
        "  model.add(layers.Dense(y_train.shape[1], activation = 'softmax'))\n",
        "  model.compile(optimizer='adam',loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK0KWjTEyTZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "157df7de-bfc1-410c-f81d-7f2f551fe184"
      },
      "source": [
        "model = getmodel()\n",
        "history = model.fit(x_train_partial,\n",
        "                    y_train_partial,\n",
        "                    epochs=30,\n",
        "                    batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, None, 128)         128000    \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, None, 1026)        4740120   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, None, 1026)        0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 504)               3086496   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1000)              505000    \n",
            "=================================================================\n",
            "Total params: 8,459,616\n",
            "Trainable params: 8,459,616\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "309370/309370 [==============================] - 90s 291us/step - loss: 5.9028 - accuracy: 0.0547\n",
            "Epoch 2/30\n",
            "309370/309370 [==============================] - 88s 286us/step - loss: 5.6585 - accuracy: 0.0682\n",
            "Epoch 3/30\n",
            "309370/309370 [==============================] - 88s 284us/step - loss: 5.5038 - accuracy: 0.0809\n",
            "Epoch 4/30\n",
            "309370/309370 [==============================] - 87s 281us/step - loss: 5.4001 - accuracy: 0.0907\n",
            "Epoch 5/30\n",
            "309370/309370 [==============================] - 88s 285us/step - loss: 5.3168 - accuracy: 0.0966\n",
            "Epoch 6/30\n",
            "309370/309370 [==============================] - 88s 286us/step - loss: 5.2359 - accuracy: 0.1015\n",
            "Epoch 7/30\n",
            "309370/309370 [==============================] - 87s 281us/step - loss: 5.1539 - accuracy: 0.1057\n",
            "Epoch 8/30\n",
            "309370/309370 [==============================] - 87s 280us/step - loss: 5.0700 - accuracy: 0.1102\n",
            "Epoch 9/30\n",
            "309370/309370 [==============================] - 87s 283us/step - loss: 4.9824 - accuracy: 0.1145\n",
            "Epoch 10/30\n",
            "309370/309370 [==============================] - 87s 281us/step - loss: 4.8914 - accuracy: 0.1201\n",
            "Epoch 11/30\n",
            "309370/309370 [==============================] - 87s 281us/step - loss: 4.7954 - accuracy: 0.1272\n",
            "Epoch 12/30\n",
            "309370/309370 [==============================] - 88s 283us/step - loss: 4.6953 - accuracy: 0.1351\n",
            "Epoch 13/30\n",
            "309370/309370 [==============================] - 87s 281us/step - loss: 4.5893 - accuracy: 0.1453\n",
            "Epoch 14/30\n",
            "309370/309370 [==============================] - 86s 279us/step - loss: 4.4775 - accuracy: 0.1563\n",
            "Epoch 15/30\n",
            "309370/309370 [==============================] - 87s 280us/step - loss: 4.3604 - accuracy: 0.1694\n",
            "Epoch 16/30\n",
            "309370/309370 [==============================] - 86s 278us/step - loss: 4.2419 - accuracy: 0.1839\n",
            "Epoch 17/30\n",
            "309370/309370 [==============================] - 87s 282us/step - loss: 4.1177 - accuracy: 0.1999\n",
            "Epoch 18/30\n",
            "309370/309370 [==============================] - 88s 283us/step - loss: 3.9932 - accuracy: 0.2178\n",
            "Epoch 19/30\n",
            "309370/309370 [==============================] - 88s 283us/step - loss: 3.8657 - accuracy: 0.2366\n",
            "Epoch 20/30\n",
            "309370/309370 [==============================] - 89s 287us/step - loss: 3.7405 - accuracy: 0.2550\n",
            "Epoch 21/30\n",
            "309370/309370 [==============================] - 89s 287us/step - loss: 3.6164 - accuracy: 0.2755\n",
            "Epoch 22/30\n",
            "309370/309370 [==============================] - 88s 284us/step - loss: 3.4947 - accuracy: 0.2966\n",
            "Epoch 23/30\n",
            "309370/309370 [==============================] - 87s 280us/step - loss: 3.3765 - accuracy: 0.3167\n",
            "Epoch 24/30\n",
            "309370/309370 [==============================] - 86s 278us/step - loss: 3.2612 - accuracy: 0.3371\n",
            "Epoch 25/30\n",
            "309370/309370 [==============================] - 86s 277us/step - loss: 3.1522 - accuracy: 0.3559\n",
            "Epoch 26/30\n",
            "309370/309370 [==============================] - 85s 276us/step - loss: 3.0482 - accuracy: 0.3752\n",
            "Epoch 27/30\n",
            "309370/309370 [==============================] - 85s 275us/step - loss: 2.9463 - accuracy: 0.3942\n",
            "Epoch 28/30\n",
            "309370/309370 [==============================] - 85s 276us/step - loss: 2.8511 - accuracy: 0.4123\n",
            "Epoch 29/30\n",
            "309370/309370 [==============================] - 84s 272us/step - loss: 2.7602 - accuracy: 0.4287\n",
            "Epoch 30/30\n",
            "309370/309370 [==============================] - 84s 272us/step - loss: 2.6736 - accuracy: 0.4454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ck-tY2TKSRz"
      },
      "source": [
        "reverse_word_index = {index:word for word,index in tokenizer.word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTHI56HtLjR-"
      },
      "source": [
        "def predict(input_lines):\n",
        "    input_lines = [line.strip().lower() + ' end' for line in input_lines]\n",
        "    lines = tokenizer.texts_to_sequences(input_lines)\n",
        "    input_x,input_y = partitionlines(lines,maxlen)\n",
        "    model_input_x = sequence.pad_sequences(input_x,maxlen=maxlen)\n",
        "    output_y = model.predict(model_input_x)\n",
        "    output_seq = np.argmax(output_y,axis = 0)\n",
        "    output_lines = [[reverse_word_index[num] for num in inp] for inp in input_x]\n",
        "    predicted_words = [reverse_word_index[num] for num in output_seq]\n",
        "    expected_words = [reverse_word_index[num] for num in input_y]\n",
        "    for i in range(0,len(input_lines)):\n",
        "        print (\"input = :{0}, predicted = {1},expected = {2}\", output_lines[i],predicted_words[i],expected_words[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebSNhqTDnqgW"
      },
      "source": [
        "def wordfromonehot(onehot):\n",
        "    index = np.argmax(onehot)\n",
        "    return reverse_word_index[index]\n",
        "def predict(input_line, count ):\n",
        "    input_line_clean = input_line.strip().lower()\n",
        "    #print(input_lines)\n",
        "    predicted_words = []\n",
        "    input_x= tokenizer.texts_to_sequences([input_line_clean])[0]\n",
        "    for _ in range(0,count):\n",
        "        model_input_x = sequence.pad_sequences([input_x],maxlen=maxlen)\n",
        "        output_y = model.predict(model_input_x)\n",
        "        #print (np.max(output_y))\n",
        "        input_x.append(np.argmax(output_y))\n",
        "        predicted_words.append((wordfromonehot(output_y), np.max(output_y)))\n",
        "    #print(predicted_words)\n",
        "    print (\"input = {0}, predicted = {1}\".format(input_line,predicted_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpwOk2xnn9e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76efbf92-dd33-434d-fbbb-26606c6f3b5a"
      },
      "source": [
        "predict(\"بازی\",10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = بازی, predicted = [('ایران', 0.2510396), ('و', 0.71295506), ('ژاپن', 0.5712498), ('تو', 0.30330458), ('در', 0.13400902), ('یه', 0.04954507), ('و', 0.49076903), ('دو', 0.12212429), ('تا', 0.41119966), ('دو', 0.1203389)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i0TAKRWpHvA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFzGDR4YoCaI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e9849284-7a0b-49a5-d19c-d62a1ee78343"
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 345us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.9973150177001955, 0.11299999803304672]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSbmQp1boFGj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "c41aa832-dd76-47da-f86b-ce49a2056951"
      },
      "source": [
        "x_test_words = []\n",
        "for seq in x_test:\n",
        "    x_test_words.append(' '.join([reverse_word_index[num] for num in seq if num != 0 ]))\n",
        "pred_test = model.predict(x_test)\n",
        "\n",
        "y_test_pred_words = [wordfromonehot(pred) for pred in pred_test]\n",
        "y_act = [wordfromonehot(y) for y in y_test]\n",
        "for i in range(100,110):\n",
        "    #print('input = {0}, pred  = {1}, actual = {2}'.format(x_test_words[i],y_test_pred_words[i],y_act[i]))\n",
        "    predict(x_test_words[i],4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = تنها احمق بلکه هم هستن مرگ بر خامنه ای مرگ, predicted = [('بر', 0.70684975), ('مرگ', 0.787247), ('بر', 0.97975105), ('جمهوری', 0.32078066)]\n",
            "input = اسرائیل که برای یک به تا رنگ دم اون رو, predicted = [('به', 0.09796313), ('کسی', 0.11500972), ('نه', 0.22940162), ('اینکه', 0.27000952)]\n",
            "input = rt رئیس ندارد و همه دست, predicted = [('و', 0.9425953), ('و', 0.49957645), ('زندگی', 0.1665816), ('مردم', 0.460238)]\n",
            "input = بی خبری از و روز بی خبری از روز بی, predicted = [('و', 0.38891712), ('از', 0.22019644), ('۱۰', 0.12494301), ('تا', 0.106594)]\n",
            "input = درد و داشتن بد این را و گفت را و, predicted = [('را', 0.22344571), ('در', 0.39821452), ('می', 0.20042418), ('و', 0.54837537)]\n",
            "input = rt پس, predicted = [('از', 0.6341409), ('سال', 0.13642953), ('از', 0.28422263), ('ایجاد', 0.6309868)]\n",
            "input = rt باشید که هیچ چیز را به, predicted = [('می', 0.092724554), ('تمام', 0.058502473), ('زنان', 0.09503929), ('را', 0.24515858)]\n",
            "input = دلیل انتخاب لهستان برای دلیل انتخاب لهستان, predicted = [('در', 0.686497), ('سوریه', 0.42658982), ('به', 0.38754505), ('زیر', 0.07610931)]\n",
            "input = رو و زمین خورد از اون روز به بعد همه, predicted = [('چی', 0.24376452), ('بده', 0.14009982), ('به', 0.27149808), ('خاطر', 0.09223553)]\n",
            "input = rt این دختر بچه, predicted = [('ساله', 0.86120254), ('در', 0.9928067), ('مقابل', 0.95125705), ('ی', 0.8348639)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj9AOr7roHfZ"
      },
      "source": [
        "model.save('outlook_pred.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PRcy5nsvXWc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}